{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.sparse import FloatTensor as STensor\n",
    "from torch.cuda.sparse import FloatTensor as CudaSTensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from datareader import ShotsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "def options():\n",
    "  parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "  parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                      help='input batch size for training (default: 64)')\n",
    "  parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                      help='input batch size for testing (default: 1000)')\n",
    "  parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                      help='number of epochs to train (default: 10)')\n",
    "  parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                      help='learning rate (default: 0.01)')\n",
    "  parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                      help='SGD momentum (default: 0.5)')\n",
    "  parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                      help='disables CUDA training')\n",
    "  parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                      help='random seed (default: 1)')\n",
    "  parser.add_argument('--vis-scalar-freq', type=int, default=10, metavar='N',\n",
    "                      help='how many batches to wait before logging training status')\n",
    "  args = parser.parse_args([\"--lr\",\"1e-3\", \"--no-cuda\"])\n",
    "  args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = options()\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    ShotsDataset(\"/cs/ml/datasets/bball/v1/bball_tracking\",res_bh=10, res_def=2),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    ShotsDataset(\"/cs/ml/datasets/bball/v1/bball_tracking\", res_bh=10, res_def=2),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx_to_one_hot(idx, batch_size, feat_dim, cuda=False):\n",
    "    T = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "    ST = CudaSTensor if cuda else STensor\n",
    "\n",
    "    batch_idx = T([i for i in range(batch_size)])\n",
    "    feat_idx = idx.view(batch_size,)\n",
    "\n",
    "    my_stack = torch.stack([batch_idx, feat_idx]) # indices must be nDim * nEntries\n",
    "    ones = torch.ones(batch_size)\n",
    "    if cuda:\n",
    "        ones = ones.type(torch.cuda.FloatTensor)\n",
    "    y = ST(my_stack, ones, torch.Size([batch_size, feat_dim])).to_dense()\n",
    "\n",
    "    return y\n",
    "\n",
    "def idx_to_multi_hot(idx, batch_size, feat_dim, cuda=False):\n",
    "    y_ = idx_to_one_hot(idx, batch_size, feat_dim, cuda=cuda)\n",
    "    y = torch.sum(y_, 1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorModel(nn.Module):\n",
    "    def __init__(self, args, dims, test=False):\n",
    "        super(TensorModel, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.n_classes = 2\n",
    "\n",
    "        self.dK = dims[\"K\"]\n",
    "\n",
    "        # Scaling factor = 10\n",
    "        self.dA,self.dB,self.dC = dims[\"A\"], dims[\"B\"], dims[\"C\"]\n",
    "\n",
    "        T = torch.cuda if self.args.cuda else torch\n",
    "\n",
    "        self.f_bh = T.FloatTensor(args.batch_size, self.dB)\n",
    "        self.f_def = T.FloatTensor(args.batch_size, self.dC)\n",
    "\n",
    "        self._F = torch.FloatTensor(self.dA,self.dB,self.dC).zero_()\n",
    "        self._F = Parameter(self._F, requires_grad=True)\n",
    "\n",
    "        self._A = torch.FloatTensor(self.dA,self.dK).zero_()\n",
    "        self._A = Parameter(self._A, requires_grad=True)\n",
    "        self._B = torch.FloatTensor(self.dB,self.dK).zero_()\n",
    "        self._B = Parameter(self._B, requires_grad=True)\n",
    "        self._C = torch.FloatTensor(self.dC,self.dK).zero_()\n",
    "        self._C = Parameter(self._C, requires_grad=True)\n",
    "\n",
    "        self.init_random()\n",
    "\n",
    "    def init_random(self):\n",
    "        self._F.data.normal_(std=0.1)\n",
    "        self._A.data.normal_(std=0.1)\n",
    "        self._B.data.normal_(std=0.1)\n",
    "        self._C.data.normal_(std=0.1)\n",
    "        \n",
    "        print(self._F.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.f_bh.zero_()\n",
    "        idx = torch.unsqueeze(x[:,0].data, 1) % self.dB\n",
    "        self.f_bh.scatter_(1, idx, 1)\n",
    "\n",
    "        self.f_def.zero_()\n",
    "        for i in range(5):\n",
    "          idx = torch.unsqueeze(x[:,i].data, 1) % self.dC\n",
    "          self.f_def.scatter_(1, idx, 1)\n",
    "\n",
    "        v_f_bh = Variable(self.f_bh, requires_grad=False)\n",
    "        v_f_def = Variable(self.f_def, requires_grad=False)\n",
    "\n",
    "        # f_bh [B x dB] -> [B x 1 x 1 x dB]\n",
    "        v_f_bh = torch.unsqueeze(v_f_bh, 1)\n",
    "        v_f_bh = torch.unsqueeze(v_f_bh, 2)\n",
    "\n",
    "        # f_bh * F --> [B x dA x 1 x dC]\n",
    "        x = torch.matmul(v_f_bh, self._F)\n",
    "        x = torch.squeeze(x)\n",
    "\n",
    "        # (f_bh * F) * f_def *\n",
    "        # [B x dA x dC] * [B x dC x 1] = [B x dA x 1]\n",
    "        v_f_def = torch.unsqueeze(v_f_def, 2)\n",
    "\n",
    "        x = torch.bmm(x, v_f_def)\n",
    "        x = torch.squeeze(x)\n",
    "\n",
    "        # output is [B x dA]\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):        \n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        idA = target[:,0]\n",
    "        output = model(data)\n",
    "        \n",
    "        conj_output = 1 - output\n",
    "        output = torch.cat([output, conj_output], 1)\n",
    "                            \n",
    "        label_01 = target[:,2]\n",
    "        loss = F.nll_loss(output, label_01)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        \n",
    "        idA = target[:,0]\n",
    "        label_01 = target[:,2]\n",
    "        output = torch.gather(output, 1, idA)\n",
    "        \n",
    "        conj_output = 1 - output\n",
    "        output = torch.cat([output, conj_output], 1)\n",
    "        \n",
    "        test_loss += F.nll_loss(output, label_01, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( 0 ,.,.) = \n",
      " -9.4955e-02 -1.6301e-01  6.6675e-02  ...  -1.5309e-01  3.6662e-02  5.8938e-02\n",
      " -2.0559e-03  1.1344e-01 -1.0513e-02  ...   3.7522e-02 -9.5171e-02 -2.1418e-01\n",
      " -1.4244e-01 -1.4692e-01  1.1340e-01  ...  -4.7083e-02 -6.2154e-03  5.3985e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  6.3456e-02 -1.0137e-02 -2.1284e-01  ...   8.4366e-02  1.0488e-01  9.2674e-02\n",
      " -3.5145e-02  1.5803e-02 -6.0672e-02  ...   9.9506e-02 -1.1886e-01  9.8183e-02\n",
      "  1.6806e-01 -1.1440e-01 -9.0875e-02  ...  -7.8800e-02  7.5088e-02 -5.4279e-02\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  7.0676e-02  1.7670e-01 -1.3909e-01  ...  -1.4632e-02  7.4488e-02 -2.1645e-02\n",
      " -3.5472e-03  4.9908e-03 -9.8495e-02  ...  -7.3550e-02  1.0628e-01 -2.2763e-03\n",
      " -4.6616e-02 -1.4708e-01  1.4968e-02  ...  -3.0381e-02  2.1769e-01 -1.1509e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -2.6585e-02  1.0834e-02  7.4253e-02  ...  -1.2562e-01  1.5431e-02  9.3649e-02\n",
      " -1.7755e-01 -8.2570e-04  9.3630e-02  ...  -3.3580e-02 -2.3632e-02  2.1852e-01\n",
      " -6.5845e-03  2.1677e-03  6.6451e-02  ...  -8.7827e-02  1.2353e-01  1.6711e-02\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  1.0838e-02 -8.5936e-02  1.0108e-01  ...   4.6018e-02 -4.2327e-02 -8.4129e-02\n",
      " -1.3790e-01  3.9992e-02 -5.8204e-02  ...   3.7911e-02  1.3485e-01  2.3951e-02\n",
      " -2.5904e-01  1.2669e-01 -2.4031e-02  ...  -1.5217e-02  1.3068e-01 -1.3655e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.4124e-01 -2.5669e-01  3.3129e-02  ...  -2.7867e-03 -1.0427e-01 -1.4182e-01\n",
      " -6.1376e-02 -1.0248e-01 -1.6878e-02  ...   2.3929e-01  1.2753e-01 -1.4808e-02\n",
      "  1.0773e-01  1.0932e-02 -3.0696e-02  ...  -2.1993e-02  3.3145e-02 -1.4377e-02\n",
      "... \n",
      "\n",
      "(436,.,.) = \n",
      "  6.9798e-02 -9.2925e-02 -5.7001e-03  ...   3.6649e-02 -5.2408e-02  9.8766e-02\n",
      "  1.1535e-01 -8.6366e-02 -7.6695e-02  ...  -1.6377e-01  4.8846e-02  6.9527e-02\n",
      "  2.8413e-02  1.9013e-02 -4.1922e-02  ...  -8.2947e-02 -1.6885e-01 -9.2035e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.4032e-01  8.3932e-02 -4.5478e-02  ...  -8.6829e-02  1.0749e-01 -1.1018e-01\n",
      "  1.2754e-02 -4.0230e-02 -1.9745e-02  ...   2.0752e-01 -2.0290e-01  3.1081e-02\n",
      "  1.7391e-02  1.3814e-01  1.1478e-02  ...  -6.2763e-02  1.1238e-01  1.3448e-01\n",
      "\n",
      "(437,.,.) = \n",
      "  9.5543e-03 -5.7039e-02 -9.5989e-03  ...  -1.8488e-01  5.2619e-03 -3.6686e-02\n",
      "  1.8316e-01  8.6837e-02  4.4298e-02  ...  -4.8502e-02 -2.9053e-02  2.5817e-02\n",
      " -1.0422e-01  1.0549e-01 -3.3821e-02  ...  -7.5271e-02 -1.2961e-01 -1.3427e-01\n",
      "                 ...                   ⋱                   ...                \n",
      " -4.7118e-02 -1.2900e-01  1.7500e-01  ...   1.0833e-01  1.6306e-01 -2.8711e-02\n",
      "  2.7158e-02 -9.7537e-02 -1.1726e-01  ...  -1.5986e-02 -7.9133e-02 -1.8352e-01\n",
      "  1.5739e-02  2.1514e-01 -3.2571e-02  ...   1.5266e-02 -1.2194e-01 -1.9729e-01\n",
      "\n",
      "(438,.,.) = \n",
      " -4.1854e-02  4.4295e-02  2.2325e-02  ...  -1.7349e-01 -1.2272e-02 -3.6894e-02\n",
      " -6.5776e-02 -3.7966e-02 -1.3194e-01  ...   1.5308e-01  6.0068e-02 -7.8550e-03\n",
      " -1.2648e-01 -9.1359e-02 -5.9324e-02  ...   1.1959e-01  4.6411e-02 -2.0360e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -5.2009e-02  1.7563e-01 -5.7245e-02  ...  -1.5573e-01 -1.0205e-01  2.8101e-03\n",
      " -1.0411e-01 -5.1794e-03 -1.6367e-02  ...  -4.9022e-02  1.4588e-01  9.7496e-02\n",
      " -5.5255e-02 -1.1229e-01 -3.4471e-02  ...  -3.7482e-02  2.7411e-01  5.0165e-02\n",
      "[torch.FloatTensor of size 439x20x37]\n",
      "\n",
      "Train Epoch: 1 [0/15740724 (0%)]\tLoss: -0.493899\n",
      "Train Epoch: 1 [640/15740724 (0%)]\tLoss: -0.501640\n",
      "Train Epoch: 1 [1280/15740724 (0%)]\tLoss: -0.487737\n",
      "Train Epoch: 1 [1920/15740724 (0%)]\tLoss: -0.493364\n",
      "Train Epoch: 1 [2560/15740724 (0%)]\tLoss: -0.498905\n",
      "Train Epoch: 1 [3200/15740724 (0%)]\tLoss: -0.497731\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f485e7c7cdcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-033f7787037a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlabel_01\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/anaconda3/envs/explore-ma/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/anaconda3/envs/explore-ma/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dims = {\"A\":439,\"B\":20,\"C\":36+1,\"K\":2}\n",
    "    model = TensorModel(args, dims)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
